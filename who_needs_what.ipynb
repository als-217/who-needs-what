{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6a2c0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, json\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import spacy, nltk\n",
    "from nltk import pos_tag, sent_tokenize\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "acef58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    try:\n",
    "        # UCS-4\n",
    "        highpoints = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "    except re.error:\n",
    "        # UCS-2\n",
    "        highpoints = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "\n",
    "    text = highpoints.sub('', text)\n",
    "    try: \n",
    "        highpoints = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "    except:\n",
    "        highpoints = re.compile(u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')\n",
    "\n",
    "    return highpoints.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8ceddaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob(os.path.join(\"gsDataAll/gsDataEn\", \"*.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086f4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEED_VERBS = [\"need\", \"needs\", \"needed\", \"needing\", \"necessitate\", \"necessitates\", \"necessitated\", \"necessitating\",\n",
    "               \"require\", \"requires\", \"required\", \"requiring\", \"asking for\", \"ask for\", \"asks for\", \"asked for\",\n",
    "               \"want\", \"wants\", \"wanted\", \"wanting\", \"demanding\", \"demand\", \"demands\", \"demanded\", \"request\",\n",
    "               \"requests\", \"requesting\", \"requested\"]\n",
    "ASK_VERBS = [\"ask\", \"asks\", \"asked\", \"asking\"]\n",
    "NEED_NOUNS = [\"need\", \"needs\", \"want\", \"wants\", \"requirement\", \"requirements\", \"necessity\", \"necessities\",\n",
    "              \"request\", \"requests\", \"demands\", \"demand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e0cb8361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 863/863 [42:30<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "sents = {}\n",
    "for file in tqdm(files):\n",
    "    day = np.load(file, allow_pickle=True)\n",
    "    tweets = pd.json_normalize(day)\n",
    "    to_drop = [i for i, tweet in tweets.iterrows() if any(word in tweet[\"text\"].lower() for word in NEED_VERBS) == False]\n",
    "    tweets = tweets.drop(to_drop)\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: ' '.join(re.sub(\"([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)\",\" \", x).split()))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_emojis(x))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'\\$\\w*', '', x))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'#', '', x))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub('&amp;', '', x))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'https?:\\/\\/.*\\/\\w*', ' ', x))\n",
    "    sents_for_file = [sent_tokenize(tweet) for tweet in tweets[\"text\"]]\n",
    "    sents[file] = sents_for_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a998e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\"ner\", \"tok2vec\", \"lemmatizer\", \"textcat\", \"senter\",\n",
    "                                      \"sentencizer\", \"attribute_ruler\", \"trainable_lemmatizer\",\n",
    "                                      \"textcat_multilabel\", \"entity_ruler\", \"entity_linker\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e69b0eeb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [00:05<00:00,  1.53it/s]\n",
      " 22%|█████████▊                                  | 8/36 [00:03<00:13,  2.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mcombinations(options, num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))):\n\u001b[1;32m      4\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     pos_tokenized \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, file \u001b[38;5;129;01min\u001b[39;00m sents\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:432\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:468\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/en_core_web_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:649\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:514\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    505\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m    506\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    507\u001b[0m     config,\n\u001b[1;32m    508\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    513\u001b[0m )\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:2125\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(exclude) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2125\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m path  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link_components()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:1352\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[0;32m-> 1352\u001b[0m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:2101\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.deserialize_vocab\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeserialize_vocab\u001b[39m(path: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m-> 2101\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/vocab.pyx:492\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/vectors.pyx:629\u001b[0m, in \u001b[0;36mspacy.vectors.Vectors.from_disk\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:1343\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1339\u001b[0m             writer(path \u001b[38;5;241m/\u001b[39m key)\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_disk\u001b[39m(\n\u001b[1;32m   1344\u001b[0m     path: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   1345\u001b[0m     readers: Dict[\u001b[38;5;28mstr\u001b[39m, Callable[[Path], \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[1;32m   1346\u001b[0m     exclude: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   1347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[1;32m   1348\u001b[0m     path \u001b[38;5;241m=\u001b[39m ensure_path(path)\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1350\u001b[0m         \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "possibilities = []\n",
    "for num in range(11):\n",
    "    for subset in tqdm(list(itertools.combinations(options, num + 1))):\n",
    "        start = time.time()\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=subset)\n",
    "        pos_tokenized = []\n",
    "        for key, file in sents.items():\n",
    "            for idx, tweet in enumerate(file):\n",
    "                if idx == 5:\n",
    "                    end = time.time() - start\n",
    "                    break\n",
    "                for sent in tweet:\n",
    "                    string = string.replace(\"[^\\w\\d'\\s-]+\", ' ')\n",
    "                    doc = nlp(re.sub(string)\n",
    "                    pos_tokenized.append([{\"Text\": token.text, \"POS\": token.pos_,\n",
    "                                   \"Dep\": token.dep_, \"Tag\": token.tag_, \n",
    "                                   \"Children\": [str(child) for child in token.children]} for token in doc])\n",
    "        if pos_tokenized[0][19][\"POS\"] != '' and pos_tokenized[0][19][\"Dep\"] != '' and len(pos_tokenized[0][19][\"Children\"]) == 2:\n",
    "            possibilities.append({\"Options\": subset, \"ItPerSecond\": 1 / (end / 5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "161368f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastest = min([subset[\"ItPerSecond\"] for subset in possibilities])\n",
    "options = [subset[\"Options\"] for subset in possibilities if subset[\"ItPerSecond\"] == fastest][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "84cdb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 863/863 [3:15:14<00:00, 13.57s/it]\n"
     ]
    }
   ],
   "source": [
    "options = ('ner', 'lemmatizer', 'textcat', 'senter', 'sentencizer', 'trainable_lemmatizer', 'textcat_multilabel', 'entity_ruler', 'entity_linker')\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=options)\n",
    "pos_tokenized = []\n",
    "for key, file in tqdm(sents.items()):\n",
    "    for idx, tweet in enumerate(file):\n",
    "        for sent in tweet:\n",
    "            string = sent.replace(r\"[^\\w\\d'\\s-]+\", ' ')\n",
    "            string = \" \".join(string.split())\n",
    "            # doc = nlp(string)\n",
    "            # string = string.replace(\"/\\s\\s+/g\", ' ')\n",
    "            doc = nlp(re.sub(r\"[^\\w\\d'\\s-]+\", '', string))\n",
    "            pos_tokenized.append([{\"Text\": token.text, \"POS\": token.pos_,\n",
    "                           \"Dep\": token.dep_, \"Tag\": token.tag_, \n",
    "                           \"Children\": [str(child) for child in token.children]} for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b40a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEED_VERBS = [\"need\", \"needs\", \"needed\", \"needing\", \"necessitate\", \"necessitates\", \"necessitated\", \"necessitating\",\n",
    "               \"require\", \"requires\", \"required\", \"requiring\", \"want\", \"wants\", \"wanted\", \"wanting\",\n",
    "               \"demanding\", \"demand\", \"demands\", \"demanded\", \"request\", \"requests\", \"requesting\", \"requested\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951eafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_needs_what(sent: list):\n",
    "    need_term = ''\n",
    "    for idx, word in enumerate(sent):\n",
    "        if (word[\"Text\"] in NEED_VERBS or word[\"Text\"] in ASK_VERBS) and word[\"POS\"] == \"VERB\":\n",
    "            need_term = word[\"Text\"]\n",
    "            what = verb_find_what(sent, idx)\n",
    "            who = verb_find_who(sent, idx)\n",
    "            break\n",
    "        elif word[\"Text\"] in NEED_NOUNS and word[\"POS\"] == \"NOUN\":\n",
    "            need_term = word[\"Text\"]\n",
    "            what = noun_find_what(sent, idx)\n",
    "            who = noun_find_who(sent, idx)\n",
    "            break\n",
    "            \n",
    "    return {\"Who\": who, \"Need-Term\": need_term, \"What\": what, \"Need-POS\": word[\"POS\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eac649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(sent: list, cur_idx: int) -> str:\n",
    "    adj = ''\n",
    "    i = 1\n",
    "    while(cur_idx-i >= 0):\n",
    "        if sent[cur_idx-i][\"Dep\"] == \"compound\" or sent[cur_idx-i][\"Dep\"] == \"amod\":\n",
    "            adj = (sent[cur_idx-i][\"Text\"] + ' ') + adj\n",
    "        else:\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42aaaa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_find_what(sent: list, word_idx: int):\n",
    "    adj = ''\n",
    "    what = ''\n",
    "    for cur_idx, word in enumerate(sent[word_idx:]):\n",
    "        if (word[\"Dep\"] == \"dobj\" or word[\"Dep\"] == \"pobj\") and word[\"POS\"] == \"NOUN\":\n",
    "            what = word[\"Text\"]\n",
    "            adj = get_adjectives(sent, cur_idx)\n",
    "        elif word[\"Dep\"] == \"dobj\" and word[\"Text\"] in sent[word_idx][\"Children\"] and word[\"POS\"] == \"PROPN\":\n",
    "            what = word[\"Text\"]\n",
    "                \n",
    "    return adj + what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058b75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_find_who(sent: list, word_idx: int):\n",
    "    who = ''\n",
    "    i = word_idx\n",
    "    while (i != -1):\n",
    "        if sent[i][\"Dep\"] == \"nsubj\" and sent[i][\"Text\"] in sent[word_idx][\"Children\"]:\n",
    "            if sent[i][\"Tag\"] == \"WDT\":\n",
    "                for word in sent[:i]:\n",
    "                    if (word[\"POS\"] == \"NOUN\" or word[\"POS\"] == \"PROPN\") and word[\"Dep\"] == \"attr\":\n",
    "                        who = word[\"Text\"]\n",
    "            else:\n",
    "                who = sent[i][\"Text\"]\n",
    "        i -= 1\n",
    "        \n",
    "    return who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b42f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_find_what(sent: list, word_idx: int):\n",
    "    what = ''\n",
    "    for cur_idx, word in enumerate(sent[word_idx:]):\n",
    "        if word[\"Dep\"] == \"pobj\" and word[\"POS\"] == \"NOUN\":\n",
    "            what = word[\"Text\"]\n",
    "            adj = get_adjectives(sent, cur_idx)\n",
    "            \n",
    "    return adj + what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87103216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_find_who(sent: list, word_idx: int):\n",
    "    who = ''\n",
    "    i = word_idx\n",
    "    dep = \"nsubj\"\n",
    "    while (i != -1):\n",
    "        if sent[i][\"Tag\"] == \"WDT\":\n",
    "            dep = \"attr\"\n",
    "        if sent[i][\"Dep\"] == dep:\n",
    "            who = sent[i][\"Text\"]\n",
    "            \n",
    "        i -= 1\n",
    "            \n",
    "    return who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "df0dd7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "triples, fails = [], []\n",
    "for i, sent in enumerate(tqdm(pos_tokenized)):\n",
    "    try:\n",
    "        triples.append((who_needs_what(sent), i)) \n",
    "    except:\n",
    "        fails.append(sent)\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"who_needs_what_output.npy\", [triple[0] for triple in triples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "524f5564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246.23144698143005"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "692a3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, totals = {}, {}\n",
    "for verb in NEED_VERBS:\n",
    "    stats[(verb, \"VERB\")] = 0\n",
    "    totals[(verb, \"VERB\")] = 0\n",
    "    \n",
    "for verb in ASK_VERBS:\n",
    "    stats[(verb, \"VERB\")] = 0\n",
    "    totals[(verb, \"VERB\")] = 0\n",
    "\n",
    "for noun in NEED_NOUNS:\n",
    "    stats[(noun, \"NOUN\")] = 0\n",
    "    totals[(noun, \"NOUN\")] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "de7c3008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 1506664/1506664 [04:56<00:00, 5080.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for triple in tqdm(triples):\n",
    "    pos = triple[0][\"Need-POS\"]\n",
    "    totals[(triple[0][\"Need-Term\"], pos)] += 1\n",
    "    if triple[0][\"What\"] != '' and triple[0][\"Who\"] != '':\n",
    "        stats[(triple[0][\"Need-Term\"], pos)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1788f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = [(value / totals[key]) if totals[key] != 0 else -1 for key, value in zip(stats.keys(), stats.values())]\n",
    "proportions = {key: proportions[i] for i, key in enumerate(stats.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127131f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(stats, \"count_by_term.json\")\n",
    "json.dump(proportions, \"prop_by_term.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0d39697",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Who': 'Ukraine',\n",
       "  'Need-Term': 'needs',\n",
       "  'What': 'guarantees',\n",
       "  'Need-POS': 'VERB'},\n",
       " 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c1ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "who_list = [sent[0][\"Who\"] for sent in triples]\n",
    "what_list = [sent[0][\"What\"] for sent in triples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23692537",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Counter(what_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "needs_venv",
   "language": "python",
   "name": "needs_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
