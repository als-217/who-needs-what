{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6a2c0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, json\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import spacy, nltk\n",
    "from nltk import pos_tag, sent_tokenize\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "acef58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    try:\n",
    "        # UCS-4\n",
    "        highpoints = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "    except re.error:\n",
    "        # UCS-2\n",
    "        highpoints = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "\n",
    "    text = highpoints.sub('', text)\n",
    "    try: \n",
    "        highpoints = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "    except:\n",
    "        highpoints = re.compile(u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')\n",
    "\n",
    "    return highpoints.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8ceddaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob(os.path.join(\"gsDataAll/gsDataEn\", \"*.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086f4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEED_VERBS = [\"need\", \"needs\", \"needed\", \"needing\", \"necessitate\", \"necessitates\", \"necessitated\", \"necessitating\",\n",
    "               \"require\", \"requires\", \"required\", \"requiring\", \"asking for\", \"ask for\", \"asks for\", \"asked for\",\n",
    "               \"want\", \"wants\", \"wanted\", \"wanting\", \"demanding\", \"demand\", \"demands\", \"demanded\", \"request\",\n",
    "               \"requests\", \"requesting\", \"requested\"]\n",
    "ASK_VERBS = [\"ask\", \"asks\", \"asked\", \"asking\"]\n",
    "NEED_NOUNS = [\"need\", \"needs\", \"want\", \"wants\", \"requirement\", \"requirements\", \"necessity\", \"necessities\",\n",
    "              \"request\", \"requests\", \"demands\", \"demand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5e55a8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 863/863 [42:30<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "sents = {}\n",
    "for file in tqdm(files):\n",
    "    day = np.load(file, allow_pickle=True)\n",
    "    tweets = pd.json_normalize(day)\n",
    "    to_drop = [i for i, tweet in tweets.iterrows() if any(word in tweet[\"text\"].lower() for word in NEED_VERBS) == False]\n",
    "    tweets = tweets.drop(to_drop)\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: ' '.join(re.sub(\"([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)\",\" \", x).split()))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_emojis(x))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'\\$\\w*', '', x))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'#', '', x))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub('&amp;', '', x))\n",
    "    tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'https?:\\/\\/.*\\/\\w*', ' ', x))\n",
    "    sents_for_file = [sent_tokenize(tweet) for tweet in tweets[\"text\"]]\n",
    "    sents[file] = sents_for_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a998e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\"ner\", \"tok2vec\", \"lemmatizer\", \"textcat\", \"senter\",\n",
    "                                      \"sentencizer\", \"attribute_ruler\", \"trainable_lemmatizer\",\n",
    "                                      \"textcat_multilabel\", \"entity_ruler\", \"entity_linker\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "possibilities = []\n",
    "for num in range(11):\n",
    "    for subset in tqdm(list(itertools.combinations(options, num + 1))):\n",
    "        start = time.time()\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=subset)\n",
    "        pos_tokenized = []\n",
    "        for key, file in sents.items():\n",
    "            for idx, tweet in enumerate(file):\n",
    "                if idx == 5:\n",
    "                    end = time.time() - start\n",
    "                    break\n",
    "                for sent in tweet:\n",
    "                    string = string.replace(\"[^\\w\\d'\\s-]+\", ' ')\n",
    "                    doc = nlp(re.sub(string)\n",
    "                    pos_tokenized.append([{\"Text\": token.text, \"POS\": token.pos_,\n",
    "                                   \"Dep\": token.dep_, \"Tag\": token.tag_, \n",
    "                                   \"Children\": [str(child) for child in token.children]} for token in doc])\n",
    "        if pos_tokenized[0][19][\"POS\"] != '' and pos_tokenized[0][19][\"Dep\"] != '' and len(pos_tokenized[0][19][\"Children\"]) == 2:\n",
    "            possibilities.append({\"Options\": subset, \"ItPerSecond\": 1 / (end / 5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "161368f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastest = min([subset[\"ItPerSecond\"] for subset in possibilities])\n",
    "options = [subset[\"Options\"] for subset in possibilities if subset[\"ItPerSecond\"] == fastest][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "84cdb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 863/863 [3:15:14<00:00, 13.57s/it]\n"
     ]
    }
   ],
   "source": [
    "options = ('ner', 'lemmatizer', 'textcat', 'senter', 'sentencizer', 'trainable_lemmatizer', 'textcat_multilabel', 'entity_ruler', 'entity_linker')\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=options)\n",
    "pos_tokenized = []\n",
    "for key, file in tqdm(sents.items()):\n",
    "    for idx, tweet in enumerate(file):\n",
    "        for sent in tweet:\n",
    "            string = sent.replace(r\"[^\\w\\d'\\s-]+\", ' ')\n",
    "            string = \" \".join(string.split())\n",
    "            # doc = nlp(string)\n",
    "            # string = string.replace(\"/\\s\\s+/g\", ' ')\n",
    "            doc = nlp(re.sub(r\"[^\\w\\d'\\s-]+\", '', string))\n",
    "            pos_tokenized.append([{\"Text\": token.text, \"POS\": token.pos_,\n",
    "                           \"Dep\": token.dep_, \"Tag\": token.tag_, \n",
    "                           \"Children\": [str(child) for child in token.children]} for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b40a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEED_VERBS = [\"need\", \"needs\", \"needed\", \"needing\", \"necessitate\", \"necessitates\", \"necessitated\", \"necessitating\",\n",
    "               \"require\", \"requires\", \"required\", \"requiring\", \"want\", \"wants\", \"wanted\", \"wanting\",\n",
    "               \"demanding\", \"demand\", \"demands\", \"demanded\", \"request\", \"requests\", \"requesting\", \"requested\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951eafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_needs_what(sent: list):\n",
    "    need_term = ''\n",
    "    for idx, word in enumerate(sent):\n",
    "        if (word[\"Text\"] in NEED_VERBS or word[\"Text\"] in ASK_VERBS) and word[\"POS\"] == \"VERB\":\n",
    "            need_term = word[\"Text\"]\n",
    "            what = verb_find_what(sent, idx)\n",
    "            who = verb_find_who(sent, idx)\n",
    "            break\n",
    "        elif word[\"Text\"] in NEED_NOUNS and word[\"POS\"] == \"NOUN\":\n",
    "            need_term = word[\"Text\"]\n",
    "            what = noun_find_what(sent, idx)\n",
    "            who = noun_find_who(sent, idx)\n",
    "            break\n",
    "            \n",
    "    return {\"Who\": who, \"Need-Term\": need_term, \"What\": what, \"Need-POS\": word[\"POS\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eac649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(sent: list, cur_idx: int) -> str:\n",
    "    adj = ''\n",
    "    i = 1\n",
    "    while(cur_idx-i >= 0):\n",
    "        if sent[cur_idx-i][\"Dep\"] == \"compound\" or sent[cur_idx-i][\"Dep\"] == \"amod\":\n",
    "            adj = (sent[cur_idx-i][\"Text\"] + ' ') + adj\n",
    "        else:\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42aaaa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_find_what(sent: list, word_idx: int):\n",
    "    adj = ''\n",
    "    what = ''\n",
    "    for cur_idx, word in enumerate(sent[word_idx:]):\n",
    "        if (word[\"Dep\"] == \"dobj\" or word[\"Dep\"] == \"pobj\") and word[\"POS\"] == \"NOUN\":\n",
    "            what = word[\"Text\"]\n",
    "            adj = get_adjectives(sent, cur_idx)\n",
    "        elif word[\"Dep\"] == \"dobj\" and word[\"Text\"] in sent[word_idx][\"Children\"] and word[\"POS\"] == \"PROPN\":\n",
    "            what = word[\"Text\"]\n",
    "                \n",
    "    return adj + what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058b75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_find_who(sent: list, word_idx: int):\n",
    "    who = ''\n",
    "    i = word_idx\n",
    "    while (i != -1):\n",
    "        if sent[i][\"Dep\"] == \"nsubj\" and sent[i][\"Text\"] in sent[word_idx][\"Children\"]:\n",
    "            if sent[i][\"Tag\"] == \"WDT\":\n",
    "                for word in sent[:i]:\n",
    "                    if (word[\"POS\"] == \"NOUN\" or word[\"POS\"] == \"PROPN\") and word[\"Dep\"] == \"attr\":\n",
    "                        who = word[\"Text\"]\n",
    "            else:\n",
    "                who = sent[i][\"Text\"]\n",
    "        i -= 1\n",
    "        \n",
    "    return who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b42f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_find_what(sent: list, word_idx: int):\n",
    "    what = ''\n",
    "    for cur_idx, word in enumerate(sent[word_idx:]):\n",
    "        if word[\"Dep\"] == \"pobj\" and word[\"POS\"] == \"NOUN\":\n",
    "            what = word[\"Text\"]\n",
    "            adj = get_adjectives(sent, cur_idx)\n",
    "            \n",
    "    return adj + what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87103216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_find_who(sent: list, word_idx: int):\n",
    "    who = ''\n",
    "    i = word_idx\n",
    "    dep = \"nsubj\"\n",
    "    while (i != -1):\n",
    "        if sent[i][\"Tag\"] == \"WDT\":\n",
    "            dep = \"attr\"\n",
    "        if sent[i][\"Dep\"] == dep:\n",
    "            who = sent[i][\"Text\"]\n",
    "            \n",
    "        i -= 1\n",
    "            \n",
    "    return who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "df0dd7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "triples, fails = [], []\n",
    "for i, sent in enumerate(tqdm(pos_tokenized)):\n",
    "    try:\n",
    "        triples.append((who_needs_what(sent), i)) \n",
    "    except:\n",
    "        fails.append(sent)\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2941e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"who_needs_what_output.npy\", [triple[0] for triple in triples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "25435e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246.23144698143005"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ad6efad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, totals = {}, {}\n",
    "for verb in NEED_VERBS:\n",
    "    stats[(verb, \"VERB\")] = 0\n",
    "    totals[(verb, \"VERB\")] = 0\n",
    "    \n",
    "for verb in ASK_VERBS:\n",
    "    stats[(verb, \"VERB\")] = 0\n",
    "    totals[(verb, \"VERB\")] = 0\n",
    "\n",
    "for noun in NEED_NOUNS:\n",
    "    stats[(noun, \"NOUN\")] = 0\n",
    "    totals[(noun, \"NOUN\")] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7949fbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 1506664/1506664 [04:56<00:00, 5080.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for triple in tqdm(triples):\n",
    "    pos = triple[0][\"Need-POS\"]\n",
    "    totals[(triple[0][\"Need-Term\"], pos)] += 1\n",
    "    if triple[0][\"What\"] != '' and triple[0][\"Who\"] != '':\n",
    "        stats[(triple[0][\"Need-Term\"], pos)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7eff69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = [(value / totals[key]) if totals[key] != 0 else -1 for key, value in zip(stats.keys(), stats.values())]\n",
    "proportions = {key: proportions[i] for i, key in enumerate(stats.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d922386",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(stats, \"count_by_term.json\")\n",
    "json.dump(proportions, \"prop_by_term.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0d39697",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Who': 'Ukraine',\n",
       "  'Need-Term': 'needs',\n",
       "  'What': 'guarantees',\n",
       "  'Need-POS': 'VERB'},\n",
       " 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c1ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "who_list = [sent[0][\"Who\"] for sent in triples]\n",
    "what_list = [sent[0][\"What\"] for sent in triples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23692537",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Counter(what_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "needs_venv",
   "language": "python",
   "name": "needs_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
